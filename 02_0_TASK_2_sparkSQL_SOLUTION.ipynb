{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:58:59.067627Z",
     "start_time": "2025-03-12T09:58:47.788860Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"SQLExcercise\")\n",
    "spark.getActiveSession()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ac471f9f50>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.140.33.222:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SQLExcercise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Go to https://spark.apache.org/docs/latest/sql-getting-started.html and https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Quickstart:-DataFrame to get some insights in coding Spark SQL. Always select 'Python' as the language.\n",
    "\n",
    "Use the Spark SQL Reference documentation to complete this excercise\n",
    "- To write dataframe operations: Python SparkSQL API: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html\n",
    "- To write pure SQL statements: Spark SQL API: https://spark.apache.org/docs/2.3.0/api/sql/index.html and https://spark.apache.org/docs/latest/sql-ref.html\n",
    "Helpfull site with examples: https://sparkbyexamples.com/pyspark/\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load employees.csv as a Spark Dataframe\n",
    "Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Extract \n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"./FileStore/tables/employees.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:04.428889Z",
     "start_time": "2025-03-12T09:58:59.075828Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Display the schema of the DataFrame\n",
    "Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.printSchema()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:06.976298Z",
     "start_time": "2025-03-12T09:59:06.968829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a temperary view of the dataset with name tbl_employees\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.createOrReplaceTempView(\"tbl_employees\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:07.054573Z",
     "start_time": "2025-03-12T09:59:07.004701Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Calculate the total number of employees in two ways:\n",
    "-   Via dataframe operations: Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Grouping-Data\n",
    "-   With a sql statement op tbl_employees: use spark.sql()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.groupBy().count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:07.863650Z",
     "start_time": "2025-03-12T09:59:07.069934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   10|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"select count(employee_id) from tbl_employees\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:08.945918Z",
     "start_time": "2025-03-12T09:59:07.895709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(employee_id)|\n",
      "+------------------+\n",
      "|                10|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the average salary of all employees in two ways:\n",
    "-   Via the dataframe operation 'select'\n",
    "-   With a sql statement ont tbl_employees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(avg(\"salary\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:09.569025Z",
     "start_time": "2025-03-12T09:59:09.046691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     4820.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"select avg(salary) from tbl_employees\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:09.950763Z",
     "start_time": "2025-03-12T09:59:09.626863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     4820.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the explain plan of the sql statement\n",
    "1. use the method explain(mode=\"extended\") on the spark.sql statement and look  at the different plans Spark created to excecute the query.\n",
    "2. Read the physical plan from bottom to top and try to match the plan with the query you wrote. (Exchange means that the data is being shuffled between the executors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"select avg(salary) from tbl_employees\").explain(mode=\"extended\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:10.162312Z",
     "start_time": "2025-03-12T09:59:10.068609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('avg('salary), None)]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "avg(salary): double\n",
      "Aggregate [avg(salary#20) AS avg(salary)#91]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "      +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [avg(salary#20) AS avg(salary)#91]\n",
      "+- Project [salary#20]\n",
      "   +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(salary#20)], output=[avg(salary)#91])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=159]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(salary#20)], output=[sum#95, count#96L])\n",
      "         +- FileScan csv [salary#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/KDG/dataai_groupproject/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Go to the SparkUI in the tab SQL/Dataframe\n",
    "1. Search for the query plans in the SparkUI SQL tab.\n",
    "2. Try to understand the excution plan.\n",
    "3. Go to https://dzone.com/articles/debugging-spark-performance-using-explain-plan to get some insights in the operators of the plan. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the highest salary in each department in two ways:\n",
    "-  Via the dataframe operation 'groupBy'\n",
    "-  With a sql statement ont tbl_employees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import max\n",
    "highestSalariesByDepartment = df.groupBy(\"department\").agg(max(\"salary\").alias(\"highest_salary\"))\n",
    "highestSalariesByDepartment.show()\n",
    "\n",
    "spark.sql(f'select department, max(salary) as highest_salary from tbl_employees group by department' ).explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:11.439162Z",
     "start_time": "2025-03-12T09:59:10.495360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "| department|highest_salary|\n",
      "+-----------+--------------+\n",
      "|         HR|          4800|\n",
      "|  Marketing|          4300|\n",
      "|Engineering|          6000|\n",
      "+-----------+--------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['department], ['department, 'max('salary) AS highest_salary#121]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "department: string, highest_salary: int\n",
      "Aggregate [department#19], [department#19, max(salary#20) AS highest_salary#121]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "      +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [department#19], [department#19, max(salary#20) AS highest_salary#121]\n",
      "+- Project [department#19, salary#20]\n",
      "   +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[department#19], functions=[max(salary#20)], output=[department#19, highest_salary#121])\n",
      "   +- Exchange hashpartitioning(department#19, 4), ENSURE_REQUIREMENTS, [plan_id=213]\n",
      "      +- HashAggregate(keys=[department#19], functions=[partial_max(salary#20)], output=[department#19, max#126])\n",
      "         +- FileScan csv [department#19,salary#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/KDG/dataai_groupproject/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<department:string,salary:int>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the total salary expenditure for each year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "totalSalaryExpenditureByYear = df.groupBy(year(\"hire_date\").alias(\"year\")).agg(sum(\"salary\").alias(\"total_salary_expenditure\"))\n",
    "totalSalaryExpenditureByYear.show()\n",
    "\n",
    "spark.sql(f\"select year(hire_date), sum(salary) as total_salary_expenditure from tbl_employees group by year(hire_date)\").explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:12.265004Z",
     "start_time": "2025-03-12T09:59:11.474179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|year|total_salary_expenditure|\n",
      "+----+------------------------+\n",
      "|2019|                    8800|\n",
      "|2021|                   10300|\n",
      "|2020|                    9200|\n",
      "|2022|                    8700|\n",
      "|2018|                    6000|\n",
      "|2023|                    5200|\n",
      "+----+------------------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['year('hire_date)], [unresolvedalias('year('hire_date), None), 'sum('salary) AS total_salary_expenditure#153]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "year(hire_date): int, total_salary_expenditure: bigint\n",
      "Aggregate [year(hire_date#21)], [year(hire_date#21) AS year(hire_date)#155, sum(salary#20) AS total_salary_expenditure#153L]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "      +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [_groupingexpression#158], [_groupingexpression#158 AS year(hire_date)#155, sum(salary#20) AS total_salary_expenditure#153L]\n",
      "+- Project [salary#20, year(hire_date#21) AS _groupingexpression#158]\n",
      "   +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[_groupingexpression#158], functions=[sum(salary#20)], output=[year(hire_date)#155, total_salary_expenditure#153L])\n",
      "   +- Exchange hashpartitioning(_groupingexpression#158, 4), ENSURE_REQUIREMENTS, [plan_id=276]\n",
      "      +- HashAggregate(keys=[_groupingexpression#158], functions=[partial_sum(salary#20)], output=[_groupingexpression#158, sum#160L])\n",
      "         +- Project [salary#20, year(hire_date#21) AS _groupingexpression#158]\n",
      "            +- FileScan csv [salary#20,hire_date#21] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/KDG/dataai_groupproject/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int,hire_date:date>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the number of employees per postal code\n",
    "Postal codes are available in the parquet file empPostalCodes\n",
    "Create a view for the parquet file and join the two datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_PC =spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./FileStore/tables/empPostalCodes.csv\")\n",
    "df_PC.createOrReplaceTempView(\"tbl_empPostalCodes\")\n",
    "df_empPerPc = spark.sql(\"select p.postal_code, count(e.employee_id) as number_of_employees from tbl_employees e inner join tbl_empPostalCodes p on e.employee_id = p.emp_id group by postal_code\")\n",
    "df_empPerPc.explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:12.808826Z",
     "start_time": "2025-03-12T09:59:12.298753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['postal_code], ['p.postal_code, 'count('e.employee_id) AS number_of_employees#182]\n",
      "+- 'Join Inner, ('e.employee_id = 'p.emp_id)\n",
      "   :- 'SubqueryAlias e\n",
      "   :  +- 'UnresolvedRelation [tbl_employees], [], false\n",
      "   +- 'SubqueryAlias p\n",
      "      +- 'UnresolvedRelation [tbl_empPostalCodes], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "postal_code: string, number_of_employees: bigint\n",
      "Aggregate [postal_code#179], [postal_code#179, count(employee_id#17) AS number_of_employees#182L]\n",
      "+- Join Inner, (employee_id#17 = cast(emp_id#178 as int))\n",
      "   :- SubqueryAlias e\n",
      "   :  +- SubqueryAlias tbl_employees\n",
      "   :     +- View (`tbl_employees`, [employee_id#17,name#18,department#19,salary#20,hire_date#21])\n",
      "   :        +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "   +- SubqueryAlias p\n",
      "      +- SubqueryAlias tbl_emppostalcodes\n",
      "         +- View (`tbl_empPostalCodes`, [emp_id#178,postal_code#179])\n",
      "            +- Relation [emp_id#178,postal_code#179] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [postal_code#179], [postal_code#179, count(employee_id#17) AS number_of_employees#182L]\n",
      "+- Project [employee_id#17, postal_code#179]\n",
      "   +- Join Inner, (employee_id#17 = cast(emp_id#178 as int))\n",
      "      :- Project [employee_id#17]\n",
      "      :  +- Filter isnotnull(employee_id#17)\n",
      "      :     +- Relation [employee_id#17,name#18,department#19,salary#20,hire_date#21] csv\n",
      "      +- Filter isnotnull(emp_id#178)\n",
      "         +- Relation [emp_id#178,postal_code#179] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[postal_code#179], functions=[count(employee_id#17)], output=[postal_code#179, number_of_employees#182L])\n",
      "   +- Exchange hashpartitioning(postal_code#179, 4), ENSURE_REQUIREMENTS, [plan_id=340]\n",
      "      +- HashAggregate(keys=[postal_code#179], functions=[partial_count(employee_id#17)], output=[postal_code#179, count#187L])\n",
      "         +- Project [employee_id#17, postal_code#179]\n",
      "            +- BroadcastHashJoin [employee_id#17], [cast(emp_id#178 as int)], Inner, BuildLeft, false\n",
      "               :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=335]\n",
      "               :  +- Filter isnotnull(employee_id#17)\n",
      "               :     +- FileScan csv [employee_id#17] Batched: false, DataFilters: [isnotnull(employee_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/KDG/dataai_groupproject/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(employee_id)], ReadSchema: struct<employee_id:int>\n",
      "               +- Filter isnotnull(emp_id#178)\n",
      "                  +- FileScan csv [emp_id#178,postal_code#179] Batched: false, DataFilters: [isnotnull(emp_id#178)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/KDG/dataai_groupproject/FileStore/tables/empPostalCodes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(emp_id)], ReadSchema: struct<emp_id:string,postal_code:string>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write the results to a DeltaTable in the spark-warehouse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_empPerPc.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employeesPerPostalCode\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:24.252310Z",
     "start_time": "2025-03-12T09:59:12.836512Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:24.928081Z",
     "start_time": "2025-03-12T09:59:24.279082Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stuff to create the excercises. Not part of the excercise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#df_PC =spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./FileStore/tables/empPostalCodes.csv\")\n",
    "#df_PC.write.format(\"parquet\").mode(\"overwrite\").save(\"./FileStore/tables/empPostalCodes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T09:59:24.978710Z",
     "start_time": "2025-03-12T09:59:24.971810Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
