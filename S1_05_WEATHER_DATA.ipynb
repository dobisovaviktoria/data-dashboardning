{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T11:20:34.665198Z",
     "start_time": "2025-04-30T07:39:05.409987Z"
    }
   },
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import ConnectionConfig as cc\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, collect_list, regexp_replace, split, trim\n",
    "\n",
    "cc.setupEnvironment()\n",
    "cc.set_connectionProfile(\"veloDB\")\n",
    "\n",
    "spark = cc.startLocalCluster(\"Weather_Data\", 4)\n",
    "spark.getActiveSession()\n",
    "\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m cc\u001B[38;5;241m.\u001B[39msetupEnvironment()\n\u001B[0;32m     10\u001B[0m cc\u001B[38;5;241m.\u001B[39mset_connectionProfile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mveloDB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mcc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstartLocalCluster\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWeather_Data\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m spark\u001B[38;5;241m.\u001B[39mgetActiveSession()\n\u001B[0;32m     15\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://archive-api.open-meteo.com/v1/archive\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mD:\\KdG\\Year 2\\Sem2\\DataAI_Sem_2\\sparkdelta\\ConnectionConfig.py:55\u001B[0m, in \u001B[0;36mstartLocalCluster\u001B[1;34m(appName, partitions)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# These are the packages that are needed for the sparksession to work with kafka and sqlserver\u001B[39;00m\n\u001B[0;32m     54\u001B[0m   builder \u001B[38;5;241m=\u001B[39m configure_spark_with_delta_pip(builder, extra_packages\u001B[38;5;241m=\u001B[39mextra_packages) \u001B[38;5;66;03m# This function adds the delta-lake package to the sparksession and adds the extra packages to all the executors.\u001B[39;00m\n\u001B[1;32m---> 55\u001B[0m   spark \u001B[38;5;241m=\u001B[39m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m spark\n",
      "File \u001B[1;32mD:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    495\u001B[0m     sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[0;32m    496\u001B[0m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[1;32m--> 497\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[0;32m    499\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[0;32m    500\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n",
      "File \u001B[1;32mD:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\pyspark\\context.py:515\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    513\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    514\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 515\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    517\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[1;32mD:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\pyspark\\context.py:203\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[0;32m    201\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_ensure_initialized(\u001B[38;5;28mself\u001B[39m, gateway\u001B[38;5;241m=\u001B[39mgateway, conf\u001B[38;5;241m=\u001B[39mconf)\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 203\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_init\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mappName\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparkHome\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpyFiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m        \u001B[49m\u001B[43menvironment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatchSize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserializer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjsc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    213\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprofiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[43m        \u001B[49m\u001B[43mudf_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    215\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmemory_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    216\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop()\n",
      "File \u001B[1;32mD:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\pyspark\\context.py:296\u001B[0m, in \u001B[0;36mSparkContext._do_init\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvironment[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    295\u001B[0m \u001B[38;5;66;03m# Create the Java SparkContext through Py4J\u001B[39;00m\n\u001B[1;32m--> 296\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc \u001B[38;5;241m=\u001B[39m jsc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[39;00m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf \u001B[38;5;241m=\u001B[39m SparkConf(_jconf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc()\u001B[38;5;241m.\u001B[39mconf())\n",
      "File \u001B[1;32mD:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\pyspark\\context.py:421\u001B[0m, in \u001B[0;36mSparkContext._initialize_context\u001B[1;34m(self, jconf)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    418\u001B[0m \u001B[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001B[39;00m\n\u001B[0;32m    419\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 421\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mJavaSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjconf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001B[0m, in \u001B[0;36mJavaClass.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1581\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCONSTRUCTOR_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1582\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1583\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1584\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1586\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1587\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1588\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fqn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1590\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1591\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mD:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Lib\\site-packages\\py4j\\protocol.py:334\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    330\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m                 \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[0;32m    333\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 334\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    335\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    336\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28mtype\u001B[39m \u001B[38;5;241m=\u001B[39m answer[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mPy4JError\u001B[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T11:20:34.668507700Z",
     "start_time": "2025-04-30T07:39:19.557446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_openmeteo_data(lat, lon, timestamp, zipcode):\n",
    "    dt = to_datetime(timestamp)\n",
    "    date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "    hour_index = dt.hour\n",
    "\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": date_str,\n",
    "        \"end_date\": date_str,\n",
    "        \"hourly\": \"temperature_2m,precipitation,weathercode\",\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Cannot fetch: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    data = response.json()\n",
    "    try:\n",
    "        temperature = data[\"hourly\"][\"temperature_2m\"][hour_index]\n",
    "        precipitation = data[\"hourly\"][\"precipitation\"][hour_index]\n",
    "        code = data[\"hourly\"][\"weathercode\"][hour_index]\n",
    "\n",
    "        weather_data = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"temperature\": temperature,\n",
    "            \"precipitation\": precipitation,\n",
    "            \"weather_code\": code,\n",
    "            \"zipCode\": int(zipcode),\n",
    "            \"weather_condition\": classify_weather(temperature, code, precipitation)\n",
    "        }\n",
    "        return weather_data\n",
    "\n",
    "    except (KeyError, IndexError):\n",
    "        print(\"No hourly weather\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_weather_condition(weather_data):\n",
    "    if not weather_data or \"main\" not in weather_data or \"weather\" not in weather_data or not weather_data[\"weather\"]:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    temperature = weather_data[\"main\"].get(\"temperature\")\n",
    "    weather_main = weather_data[\"weather\"][0].get(\"main\", \"\").lower()\n",
    "    weather_description = weather_data[\"weather\"][0].get(\"description\", \"\").lower()\n",
    "\n",
    "    # Unpleasant: Any form of precipitation\n",
    "    precipitation_keywords = [\"rain\", \"snow\", \"drizzle\", \"thunderstorm\", \"sleet\"]\n",
    "    if any(keyword in weather_main or keyword in weather_description for keyword in precipitation_keywords):\n",
    "        return \"Unpleasant\"\n",
    "\n",
    "    # Pleasant: Temperature above 15Â°C and sunny\n",
    "    if temperature is not None and temperature > 15 and weather_main == \"clear\":\n",
    "        return \"Pleasant\"\n",
    "\n",
    "    # Neutral: All other conditions\n",
    "    return \"Neutral\"\n",
    "\n",
    "def classify_weather(temp, weather_code, precipitation):\n",
    "    if temp is None or weather_code is None:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    unpleasant_codes = {61, 63, 65, 66, 67, 80, 81, 82, 95, 96, 99}  # rainy/thunderstorms\n",
    "\n",
    "    if weather_code in unpleasant_codes or (precipitation is not None and precipitation > 0):\n",
    "        return \"Unpleasant\"\n",
    "    if temp > 15 and weather_code == 0:  # clear sky\n",
    "        return \"Pleasant\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "def to_datetime(iso_timestamp):\n",
    "    return datetime.fromisoformat(str(iso_timestamp))"
   ],
   "id": "d3515186b1eae8d9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T11:20:34.668507700Z",
     "start_time": "2025-04-30T07:39:21.886427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load rides, locks, and stations data\n",
    "df_rides = (spark.read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"driver\", cc.get_Property(\"driver\"))\n",
    "            .option(\"url\", cc.get_Property(\"url\"))\n",
    "            .option(\"dbtable\", \"rides\")\n",
    "            .option(\"user\", cc.get_Property(\"username\"))\n",
    "            .option(\"password\", cc.get_Property(\"password\"))\n",
    "            .load())\n",
    "df_locks = (spark.read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"driver\", cc.get_Property(\"driver\"))\n",
    "            .option(\"url\", cc.get_Property(\"url\"))\n",
    "            .option(\"user\", cc.get_Property(\"username\"))\n",
    "            .option(\"password\", cc.get_Property(\"password\"))\n",
    "            .option(\"dbtable\", \"locks\")\n",
    "            .load())\n",
    "df_stations = (spark.read\n",
    "               .format(\"jdbc\")\n",
    "               .option(\"driver\", cc.get_Property(\"driver\"))\n",
    "               .option(\"url\", cc.get_Property(\"url\"))\n",
    "               .option(\"user\", cc.get_Property(\"username\"))\n",
    "               .option(\"password\", cc.get_Property(\"password\"))\n",
    "               .option(\"dbtable\", \"stations\")\n",
    "               .load())\n",
    "\n",
    "df_rides_with_stations = df_rides.alias(\"r\") \\\n",
    "    .join(df_locks.alias(\"l\"), col(\"r.startlockid\") == col(\"l.lockid\")) \\\n",
    "    .join(df_stations.alias(\"s\"), col(\"l.stationid\") == col(\"s.stationid\")) \\\n",
    "    .select(\n",
    "    col(\"r.rideid\"),\n",
    "    col(\"r.starttime\"),\n",
    "    col(\"s.zipcode\").alias(\"start_zipcode\"),\n",
    "    col(\"s.gpscoord\"),\n",
    "    trim(split(regexp_replace(\"s.gpscoord\", r\"[()]\", \"\"), \",\")[0]).cast(\"double\").alias(\"latitude\"),\n",
    "    trim(split(regexp_replace(\"s.gpscoord\", r\"[()]\", \"\"), \",\")[1]).cast(\"double\").alias(\"longitude\")\n",
    ")\n",
    "\n",
    "print(\"=== Schema ===\")\n",
    "df_rides_with_stations.printSchema()\n",
    "print(\"=== Data ===\")\n",
    "df_rides_with_stations.show()"
   ],
   "id": "7ce62ac5fd679796",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema ===\n",
      "root\n",
      " |-- rideid: integer (nullable = true)\n",
      " |-- starttime: timestamp (nullable = true)\n",
      " |-- start_zipcode: string (nullable = true)\n",
      " |-- gpscoord: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "=== Data ===\n",
      "+------+-------------------+-------------+-----------------+--------+---------+\n",
      "|rideid|          starttime|start_zipcode|         gpscoord|latitude|longitude|\n",
      "+------+-------------------+-------------+-----------------+--------+---------+\n",
      "|     4|2015-09-22 00:00:00|         2018|(51.2034,4.39221)| 51.2034|  4.39221|\n",
      "|     9|2015-09-22 00:00:00|         2000|(51.2187,4.40066)| 51.2187|  4.40066|\n",
      "|    12|2015-09-22 00:00:00|         2000|(51.2092,4.40293)| 51.2092|  4.40293|\n",
      "|    13|2015-09-22 00:00:00|         2610|(51.1771,4.39978)| 51.1771|  4.39978|\n",
      "|    18|2019-09-22 08:41:48|         2018|(51.2034,4.39221)| 51.2034|  4.39221|\n",
      "|    23|2019-09-22 08:23:27|         2000|(51.2187,4.40066)| 51.2187|  4.40066|\n",
      "|    26|2019-09-22 08:14:52|         2000|(51.2092,4.40293)| 51.2092|  4.40293|\n",
      "|    27|2019-09-22 08:51:13|         2610|(51.1771,4.39978)| 51.1771|  4.39978|\n",
      "|    34|2019-09-22 08:30:44|         2018|(51.2016,4.42361)| 51.2016|  4.42361|\n",
      "|    36|2019-09-22 08:08:45|         2030|(51.2402,4.40841)| 51.2402|  4.40841|\n",
      "|    42|2019-09-22 08:37:36|         2000|(51.2295,4.41138)| 51.2295|  4.41138|\n",
      "|    43|2019-09-22 08:03:38|         2140|(51.2186,4.43864)| 51.2186|  4.43864|\n",
      "|    45|2019-09-22 08:19:04|         2018|(51.1968,4.40575)| 51.1968|  4.40575|\n",
      "|    50|2019-09-22 08:25:54|         2000|(51.2295,4.41138)| 51.2295|  4.41138|\n",
      "|    53|2019-09-22 08:49:18|         2140|  (51.2039,4.452)| 51.2039|    4.452|\n",
      "|    65|2019-09-22 08:08:47|         2170|(51.2509,4.44224)| 51.2509|  4.44224|\n",
      "|    74|2019-09-22 08:49:02|         2018| (51.213,4.42164)|  51.213|  4.42164|\n",
      "|    76|2019-09-22 08:33:43|         2100|(51.2329,4.44448)| 51.2329|  4.44448|\n",
      "|    77|2019-09-22 08:24:51|         2018|   (51.2173,4.42)| 51.2173|     4.42|\n",
      "|    86|2019-09-22 08:42:03|         2060| (51.228,4.41899)|  51.228|  4.41899|\n",
      "+------+-------------------+-------------+-----------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T11:20:34.669525800Z",
     "start_time": "2025-04-30T07:39:37.069815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(\"weather\", exist_ok=True)\n",
    "\n",
    "postal_code_timestamps = df_rides_with_stations.groupBy(\"start_zipcode\") \\\n",
    "    .agg(collect_list(\"starttime\").alias(\"timestamps\"),\n",
    "         collect_list(\"latitude\").alias(\"latitudes\"),\n",
    "         collect_list(\"longitude\").alias(\"longitudes\")) \\\n",
    "    .collect()\n",
    "\n",
    "for row in postal_code_timestamps:\n",
    "    start_zipcode = row[\"start_zipcode\"]\n",
    "    timestamps = row[\"timestamps\"]\n",
    "    latitudes = row[\"latitudes\"]\n",
    "    longitudes = row[\"longitudes\"]\n",
    "\n",
    "    if len(timestamps) < 3:\n",
    "        additional_timestamps = [datetime.now() - timedelta(hours=i) for i in range(3 - len(timestamps))]\n",
    "        timestamps.extend(additional_timestamps)\n",
    "\n",
    "    for i in range(3):\n",
    "        timestamp = timestamps[i]\n",
    "        latitude = latitudes[i]\n",
    "        longitude = longitudes[i]\n",
    "        weather_data = fetch_openmeteo_data(latitude, longitude, timestamp, start_zipcode)\n",
    "        if weather_data:\n",
    "            formatted_timestamp = timestamp.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "            filename = f\"weather/{start_zipcode}_{formatted_timestamp}.json\"\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(weather_data, f, indent=4, default=str)\n",
    "            print(f\"Saved weather data for {start_zipcode} at {timestamp} to {filename}\")\n",
    "\n",
    "        time.sleep(4)\n",
    "\n",
    "spark.stop()"
   ],
   "id": "cb80c5f13cec453e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weather data for 2140 at 2019-09-22 08:03:38 to weather/2140_2019-09-22_08-03-38.json\n",
      "Saved weather data for 2140 at 2019-09-22 08:49:18 to weather/2140_2019-09-22_08-49-18.json\n",
      "Saved weather data for 2140 at 2019-09-22 08:11:14 to weather/2140_2019-09-22_08-11-14.json\n",
      "Saved weather data for 2170 at 2019-09-22 08:08:47 to weather/2170_2019-09-22_08-08-47.json\n",
      "Saved weather data for 2170 at 2019-09-22 09:10:02 to weather/2170_2019-09-22_09-10-02.json\n",
      "Saved weather data for 2170 at 2019-09-22 10:57:35 to weather/2170_2019-09-22_10-57-35.json\n",
      "Saved weather data for 2100 at 2019-09-22 08:33:43 to weather/2100_2019-09-22_08-33-43.json\n",
      "Saved weather data for 2100 at 2019-09-22 08:34:50 to weather/2100_2019-09-22_08-34-50.json\n",
      "Saved weather data for 2100 at 2019-09-22 08:36:21 to weather/2100_2019-09-22_08-36-21.json\n",
      "Saved weather data for 2060 at 2019-09-22 08:42:03 to weather/2060_2019-09-22_08-42-03.json\n",
      "Saved weather data for 2060 at 2019-09-22 08:37:23 to weather/2060_2019-09-22_08-37-23.json\n",
      "Saved weather data for 2060 at 2019-09-22 08:15:15 to weather/2060_2019-09-22_08-15-15.json\n",
      "Saved weather data for 2050 at 2019-09-22 08:36:14 to weather/2050_2019-09-22_08-36-14.json\n",
      "Saved weather data for 2050 at 2019-09-22 08:29:24 to weather/2050_2019-09-22_08-29-24.json\n",
      "Saved weather data for 2050 at 2019-09-22 10:14:53 to weather/2050_2019-09-22_10-14-53.json\n",
      "Saved weather data for 2018 at 2015-09-22 00:00:00 to weather/2018_2015-09-22_00-00-00.json\n",
      "Saved weather data for 2018 at 2019-09-22 08:41:48 to weather/2018_2019-09-22_08-41-48.json\n",
      "Saved weather data for 2018 at 2019-09-22 08:30:44 to weather/2018_2019-09-22_08-30-44.json\n",
      "Saved weather data for 2030 at 2019-09-22 08:08:45 to weather/2030_2019-09-22_08-08-45.json\n",
      "Saved weather data for 2030 at 2019-09-22 08:36:44 to weather/2030_2019-09-22_08-36-44.json\n",
      "Saved weather data for 2030 at 2019-09-22 08:43:51 to weather/2030_2019-09-22_08-43-51.json\n",
      "Saved weather data for 2020 at 2019-09-22 08:57:15 to weather/2020_2019-09-22_08-57-15.json\n",
      "Saved weather data for 2020 at 2019-09-22 09:57:25 to weather/2020_2019-09-22_09-57-25.json\n",
      "Saved weather data for 2020 at 2019-09-22 09:35:37 to weather/2020_2019-09-22_09-35-37.json\n",
      "Saved weather data for 2600 at 2019-09-22 08:27:12 to weather/2600_2019-09-22_08-27-12.json\n",
      "Saved weather data for 2600 at 2019-09-22 08:23:51 to weather/2600_2019-09-22_08-23-51.json\n",
      "Saved weather data for 2600 at 2019-09-22 08:43:43 to weather/2600_2019-09-22_08-43-43.json\n",
      "Saved weather data for 2660 at 2019-09-22 08:09:34 to weather/2660_2019-09-22_08-09-34.json\n",
      "Saved weather data for 2660 at 2019-09-22 09:26:11 to weather/2660_2019-09-22_09-26-11.json\n",
      "Saved weather data for 2660 at 2019-09-22 10:22:27 to weather/2660_2019-09-22_10-22-27.json\n",
      "Saved weather data for 2000 at 2015-09-22 00:00:00 to weather/2000_2015-09-22_00-00-00.json\n",
      "Saved weather data for 2000 at 2015-09-22 00:00:00 to weather/2000_2015-09-22_00-00-00.json\n",
      "Saved weather data for 2000 at 2019-09-22 08:23:27 to weather/2000_2019-09-22_08-23-27.json\n",
      "Saved weather data for 2610 at 2015-09-22 00:00:00 to weather/2610_2015-09-22_00-00-00.json\n",
      "Saved weather data for 2610 at 2019-09-22 08:51:13 to weather/2610_2019-09-22_08-51-13.json\n",
      "Saved weather data for 2610 at 2019-09-22 08:24:57 to weather/2610_2019-09-22_08-24-57.json\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
