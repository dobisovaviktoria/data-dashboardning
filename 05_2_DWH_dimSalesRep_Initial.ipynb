{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions\n",
    "import ConnectionConfig as cc\n",
    "from pyspark.sql.functions import *\n",
    "cc.setupEnvironment()\n",
    "cc.listEnvironment()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T08:56:24.426289Z",
     "start_time": "2025-03-12T08:56:24.329166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLUSERSPROFILE: C:\\ProgramData\n",
      "APPDATA: C:\\Users\\dobis\\AppData\\Roaming\n",
      "COMMONPROGRAMFILES: C:\\Program Files\\Common Files\n",
      "COMMONPROGRAMFILES(X86): C:\\Program Files (x86)\\Common Files\n",
      "COMMONPROGRAMW6432: C:\\Program Files\\Common Files\n",
      "COMPUTERNAME: VIKI\n",
      "COMSPEC: C:\\WINDOWS\\system32\\cmd.exe\n",
      "DRIVERDATA: C:\\Windows\\System32\\Drivers\\DriverData\n",
      "GOPATH: C:\\Users\\dobis\\go\n",
      "HOMEDRIVE: C:\n",
      "HOMEPATH: \\Users\\dobis\n",
      "IGCCSVC_DB: AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAAdGjjLHLGEWRGn9vRsCSowQAAAACAAAAAAAQZgAAAAEAACAAAAD7UhTq8CVvkaUfJ5fXTR5kgkvcIed3OfwPabt1yHYIgAAAAAAOgAAAAAIAACAAAABSFZMhyRZv+fj9Q44MNd0sMMQVbnBwNGmcsxiNFFrAcmAAAAAQjo+0swEYFhn4kypkFiEe0Z+EUeRh+XkMWaxY6J5h885R6WUpGPQjsBjQtDBiDzTJJu/Eu8HKO9rNDQ2HtHCLXjOrbciSueB80zvNehaNnexWcFALkN4Q37FMwos4go9AAAAAsBkth/vA4x8SgTkTjgM6mv3GKidgi5oDWFMyb92y29Ab+MuztrSDRGCJMXxOGeO0p8LJ8WZBYOU66GQTsD0WFw==\n",
      "IPY_INTERRUPT_EVENT: 2416\n",
      "JPY_INTERRUPT_EVENT: 2416\n",
      "JPY_PARENT_PID: 2412\n",
      "JPY_SESSION_NAME: 05_2_DWH_dimSalesRep_Initial.ipynb\n",
      "LANG: en_US.UTF-8\n",
      "LANGUAGE: \n",
      "LC_ALL: en_US.UTF-8\n",
      "LOCALAPPDATA: C:\\Users\\dobis\\AppData\\Local\n",
      "LOGONSERVER: \\\\VIKI\n",
      "NUMBER_OF_PROCESSORS: 12\n",
      "ONEDRIVE: C:\\Users\\dobis\\OneDrive - KdG\n",
      "ONEDRIVECOMMERCIAL: C:\\Users\\dobis\\OneDrive - KdG\n",
      "ONLINESERVICES: Online Services\n",
      "OS: Windows_NT\n",
      "PATH: C:\\KDG\\sparkdelta\\.venv\\Scripts;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\PuTTY\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\nodejs\\;C:\\Program Files\\Go\\bin;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Launcher\\;C:\\Users\\dobis\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\dobis\\AppData\\Local\\JetBrains\\Toolbox\\scripts;C:\\Users\\dobis\\.jdks\\temurin-20.0.2\\bin\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\dobis\\AppData\\Roaming\\npm;C:\\Users\\dobis\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\dobis\\go\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin\n",
      "PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n",
      "PLATFORMCODE: AN\n",
      "PROCESSOR_ARCHITECTURE: AMD64\n",
      "PROCESSOR_IDENTIFIER: Intel64 Family 6 Model 186 Stepping 3, GenuineIntel\n",
      "PROCESSOR_LEVEL: 6\n",
      "PROCESSOR_REVISION: ba03\n",
      "PROGRAMDATA: C:\\ProgramData\n",
      "PROGRAMFILES: C:\\Program Files\n",
      "PROGRAMFILES(X86): C:\\Program Files (x86)\n",
      "PROGRAMW6432: C:\\Program Files\n",
      "PROMPT: (.venv) $P$G\n",
      "PSMODULEPATH: %ProgramFiles%\\WindowsPowerShell\\Modules;C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\n",
      "PT8HOME: C:\\Program Files\\Cisco Packet Tracer 8.2.1\n",
      "PUBLIC: C:\\Users\\Public\n",
      "PYTHONPATH: C:\\KDG\\dataai_groupproject;C:/Users/dobis/AppData/Local/Programs/PyCharm Professional/plugins/python-ce/helpers/pydev;C:/Users/dobis/AppData/Local/Programs/PyCharm Professional/plugins/python/helpers-pro/jupyter_debug\n",
      "REGIONCODE: EMEA\n",
      "SESSIONNAME: Console\n",
      "SYSTEMDRIVE: C:\n",
      "SYSTEMROOT: C:\\WINDOWS\n",
      "TEMP: C:\\Users\\dobis\\AppData\\Local\\Temp\n",
      "TMP: C:\\Users\\dobis\\AppData\\Local\\Temp\n",
      "TOOLBOX_VERSION: 2.5.4.38621\n",
      "USERDOMAIN: VIKI\n",
      "USERDOMAIN_ROAMINGPROFILE: VIKI\n",
      "USERNAME: dobis\n",
      "USERPROFILE: C:\\Users\\dobis\n",
      "VBOX_MSI_INSTALL_PATH: C:\\Program Files\\Oracle\\VirtualBox\\\n",
      "VIRTUAL_ENV: C:\\KDG\\sparkdelta\\.venv\n",
      "WINDIR: C:\\WINDOWS\n",
      "ZES_ENABLE_SYSMAN: 1\n",
      "_OLD_VIRTUAL_PATH: C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\PuTTY\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\nodejs\\;C:\\Program Files\\Go\\bin;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Launcher\\;C:\\Users\\dobis\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\dobis\\AppData\\Local\\JetBrains\\Toolbox\\scripts;C:\\Users\\dobis\\.jdks\\temurin-20.0.2\\bin\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\dobis\\AppData\\Roaming\\npm;C:\\Users\\dobis\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\dobis\\go\\bin\n",
      "_OLD_VIRTUAL_PROMPT: $P$G\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "PYSPARK_PYTHON: python\n",
      "SPARK_HOME: C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\n",
      "HADOOP_HOME: C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\n",
      "PYSPARK_HADOOP_VERSION: 3\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-17\\\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"dimSalesRepInit\")\n",
    "spark.getActiveSession()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T08:56:34.020923Z",
     "start_time": "2025-03-12T08:56:24.431302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x153ce0a6f10>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.140.33.222:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dimSalesRepInit</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the operational database\n",
    "In order to run this demo you have to create a tutorial_op database and run the PostgreSQL_SalesOperational.sql script.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initial load\n",
    "We will create a slowly changing dimension type 2 called dimSalesRep based on a sourceTable in our operational database called dbo.salesrep. SCD2  tables demand extra care and  because we will store hirstorical values of the dimension with the help of ranges.\n",
    "This notebook will create the table and fill it with the initial data. A second notebook will be used for increments of new and changed data.\n",
    "\n",
    "This is an example of the expected output (salesRepSK is different\n",
    "```\n",
    "+----------+-------------+-------------+-----------+-------------------+-------------------+--------------------+-------+\n",
    "|salesRepID|         name|       office| salesRepSK|          scd_start|            scd_end|                 md5|current|\n",
    "+----------+-------------+-------------+-----------+-------------------+-------------------+--------------------+-------+\n",
    "|a46add1...|      Z. Jane|     New York|          0|1990-01-01 00:00:00|2100-12-12 00:00:00|303db545462092a92...|   true|\n",
    "|s1fedf1...|   P. Chapman|       Berlin|          1|1990-01-01 00:00:00|2100-12-12 00:00:00|14b094c31bf9e4149...|   true|\n",
    "|d5e6f77...|     T. Crane|     New York|          2|1990-01-01 00:00:00|2100-12-12 00:00:00|6c062f95defda9dc3...|   true|\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cc.set_connectionProfile(\"tutorial_op\")\n",
    "\n",
    "df_operational_sales_rep = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"dbtable\", \"salesrep\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"salesRepID\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 20) \\\n",
    "    .load()\n",
    "\n",
    "#Method 1 : Use the DataFrame API\n",
    "#df_dim_sales_rep = df_sales_rep.withColumn(\"salesRepSK\", expr(\"uuid()\")) \\\n",
    "#    .withColumn(\"scd_start\", lit(\"1990-01-01\").cast(\"timestamp\")) \\\n",
    "#    .withColumn(\"scd_end\", lit(\"2100-12-12\").cast(\"timestamp\")) \\\n",
    "#    .withColumn(\"md5\", md5(concat( col('name'), col('office')))) \\\n",
    "#    .withColumn(\"current\", lit(True))\n",
    "\n",
    "#Method 2 : Use SQL\n",
    "df_operational_sales_rep.createOrReplaceTempView(\"dimSalesRep\")\n",
    "df_dim_sales_rep = spark.sql(\"select uuid() as salesRepSK, *, to_timestamp('1999-01-01','yyyy-MM-dd') as scd_start, to_timestamp('2100-12-12','yyyy-MM-dd') as scd_end, md5(concat( name, office)) as md5, True as current  from dimSalesRep\")\n",
    "\n",
    "df_dim_sales_rep.printSchema()\n",
    "df_dim_sales_rep.show()\n",
    "df_dim_sales_rep.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimSalesRep\")\n",
    "\n",
    "#spark.sql(\"ALTER TABLE dimSalesRep  ADD columns (salesRepSK long GENERATED ALWAYS AS IDENTITY (START WITH 0 INCREMENT BY 1)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T08:56:36.814882Z",
     "start_time": "2025-03-12T08:56:36.593235Z"
    }
   },
   "outputs": [
    {
     "ename": "NoSectionError",
     "evalue": "No section: 'tutorial_op'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNoSectionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m cc\u001B[38;5;241m.\u001B[39mset_connectionProfile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtutorial_op\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m df_operational_sales_rep \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread \\\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjdbc\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver\u001B[39m\u001B[38;5;124m\"\u001B[39m , \u001B[43mcc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_Property\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdriver\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m) \\\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m\"\u001B[39m, cc\u001B[38;5;241m.\u001B[39mcreate_jdbc()) \\\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalesrep\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, cc\u001B[38;5;241m.\u001B[39mget_Property(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musername\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \\\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassword\u001B[39m\u001B[38;5;124m\"\u001B[39m, cc\u001B[38;5;241m.\u001B[39mget_Property(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassword\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \\\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpartitionColumn\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalesRepID\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumPartitions\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m4\u001B[39m) \\\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlowerBound\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m) \\\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupperBound\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m20\u001B[39m) \\\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;241m.\u001B[39mload()\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m#Method 1 : Use the DataFrame API\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m#df_dim_sales_rep = df_sales_rep.withColumn(\"salesRepSK\", expr(\"uuid()\")) \\\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m#    .withColumn(\"scd_start\", lit(\"1990-01-01\").cast(\"timestamp\")) \\\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     22\u001B[0m \n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m#Method 2 : Use SQL\u001B[39;00m\n\u001B[0;32m     24\u001B[0m df_operational_sales_rep\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdimSalesRep\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\KDG\\dataai_groupproject\\ConnectionConfig.py:69\u001B[0m, in \u001B[0;36mget_Property\u001B[1;34m(propertyName)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_Property\u001B[39m(propertyName):\n\u001B[1;32m---> 69\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpropertyName\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\configparser.py:797\u001B[0m, in \u001B[0;36mRawConfigParser.get\u001B[1;34m(self, section, option, raw, vars, fallback)\u001B[0m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Get an option value for a given section.\u001B[39;00m\n\u001B[0;32m    783\u001B[0m \n\u001B[0;32m    784\u001B[0m \u001B[38;5;124;03mIf `vars` is provided, it must be a dictionary. The option is looked up\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    794\u001B[0m \u001B[38;5;124;03mThe section DEFAULT is special.\u001B[39;00m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    796\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 797\u001B[0m     d \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_unify_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43msection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mvars\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NoSectionError:\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fallback \u001B[38;5;129;01mis\u001B[39;00m _UNSET:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\configparser.py:1170\u001B[0m, in \u001B[0;36mRawConfigParser._unify_values\u001B[1;34m(self, section, vars)\u001B[0m\n\u001B[0;32m   1168\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[0;32m   1169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m section \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_section:\n\u001B[1;32m-> 1170\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NoSectionError(section) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1171\u001B[0m \u001B[38;5;66;03m# Update with the entry specific variables\u001B[39;00m\n\u001B[0;32m   1172\u001B[0m vardict \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[1;31mNoSectionError\u001B[0m: No section: 'tutorial_op'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The function lit() is used when you want a fixed column value for every row. In this case scd_start, scd_end and current.\n",
    "* The function md5() performs a md5-hash function on the preferred columns. This is needed to detect scd2 changes. When one of the included columns changes, the md5-hash will change. Include all SCD2 columns in the md5-hash function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delete the spark session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T08:56:36.816890600Z",
     "start_time": "2025-02-26T13:35:18.660603Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T08:56:36.817891500Z",
     "start_time": "2025-02-26T13:34:41.721962Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
