{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T11:18:02.373723Z",
     "start_time": "2025-05-09T11:18:01.985070Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:18:32.038489Z",
     "start_time": "2025-05-09T11:18:02.418774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"DATE_DIM\",4)\n",
    "spark.getActiveSession()"
   ],
   "id": "29d452fc1a357fe6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e69c3fbfd0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.140.35.95:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DATE_DIM</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:18:39.602057Z",
     "start_time": "2025-05-09T11:18:34.620313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#EXTRACT\n",
    "\n",
    "beginDate = '2009-01-01'\n",
    "endDate = '2023-12-31'\n",
    "\n",
    "df_SQL = spark.sql(f\"select explode(sequence(to_date('{beginDate}'), to_date('{endDate}'), interval 1 day)) as calendarDate, monotonically_increasing_id() as dateSK \")\n",
    "\n",
    "\n",
    "df_SQL.createOrReplaceTempView('neededDates' )\n",
    "\n",
    "spark.sql(\"select * from neededDates\").show()"
   ],
   "id": "814df1a18dcc6809",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|calendarDate|dateSK|\n",
      "+------------+------+\n",
      "|  2009-01-01|     0|\n",
      "|  2009-01-02|     1|\n",
      "|  2009-01-03|     2|\n",
      "|  2009-01-04|     3|\n",
      "|  2009-01-05|     4|\n",
      "|  2009-01-06|     5|\n",
      "|  2009-01-07|     6|\n",
      "|  2009-01-08|     7|\n",
      "|  2009-01-09|     8|\n",
      "|  2009-01-10|     9|\n",
      "|  2009-01-11|    10|\n",
      "|  2009-01-12|    11|\n",
      "|  2009-01-13|    12|\n",
      "|  2009-01-14|    13|\n",
      "|  2009-01-15|    14|\n",
      "|  2009-01-16|    15|\n",
      "|  2009-01-17|    16|\n",
      "|  2009-01-18|    17|\n",
      "|  2009-01-19|    18|\n",
      "|  2009-01-20|    19|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:18:39.936498Z",
     "start_time": "2025-05-09T11:18:39.680074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TRANSFORM\n",
    "dimDate = spark.sql(\"\"\"\n",
    "select\n",
    "  year(calendarDate) * 10000 + month(calendarDate) * 100 + day(calendarDate) as date_sk,\n",
    "  calendarDate as date,\n",
    "  year(calendarDate) as year,\n",
    "  date_format(calendarDate, 'MMMM') as month_name,\n",
    "  month(calendarDate) as month_nr,\n",
    "  date_format(calendarDate, 'EEEE') as day_name,\n",
    "  dayofweek(calendarDate) as day_nr,\n",
    "  case\n",
    "    when weekday(calendarDate) < 5 then 'Y'\n",
    "    else 'N'\n",
    "  end as is_weekday,\n",
    "  quarter(calendarDate) as quarter\n",
    "from neededDates\n",
    "order by calendarDate\n",
    "\"\"\")\n",
    "\n",
    "dimDate.printSchema()"
   ],
   "id": "82bd049646b3797a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_sk: integer (nullable = false)\n",
      " |-- date: date (nullable = false)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- month_name: string (nullable = false)\n",
      " |-- month_nr: integer (nullable = false)\n",
      " |-- day_name: string (nullable = false)\n",
      " |-- day_nr: integer (nullable = false)\n",
      " |-- is_weekday: string (nullable = false)\n",
      " |-- quarter: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:18:41.218403Z",
     "start_time": "2025-05-09T11:18:40.467116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from pyspark.sql.functions import explode, expr, sequence,col, date_format\n",
    "df_SparkSQL = df_SQL \\\n",
    "    .withColumn(\"date_sk\", (year(col(\"calendarDate\")) * 10000 + month(col(\"calendarDate\")) * 100 + dayofmonth(col(\"calendarDate\")))) \\\n",
    "    .withColumn(\"date\", col(\"calendarDate\")) \\\n",
    "    .withColumn(\"year\", year(col(\"calendarDate\"))) \\\n",
    "    .withColumn(\"month_name\", date_format(col(\"calendarDate\"), 'MMMM')) \\\n",
    "    .withColumn(\"month_nr\", month(col(\"calendarDate\"))) \\\n",
    "    .withColumn(\"day_name\", date_format(col(\"calendarDate\"), 'EEEE')) \\\n",
    "    .withColumn(\"day_nr\", dayofweek(col(\"calendarDate\"))) \\\n",
    "    .withColumn(\"is_weekday\", when(dayofweek(col(\"calendarDate\")).between(2, 6), \"Y\").otherwise(\"N\")) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"calendarDate\"))) \\\n",
    "    .orderBy(col(\"date\"))\n",
    "\n",
    "df_SparkSQL.show()"
   ],
   "id": "1c42bf21868685a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------+----------+----+----------+--------+---------+------+----------+-------+\n",
      "|calendarDate|dateSK| date_sk|      date|year|month_name|month_nr| day_name|day_nr|is_weekday|quarter|\n",
      "+------------+------+--------+----------+----+----------+--------+---------+------+----------+-------+\n",
      "|  2009-01-01|     0|20090101|2009-01-01|2009|   January|       1| Thursday|     5|         Y|      1|\n",
      "|  2009-01-02|     1|20090102|2009-01-02|2009|   January|       1|   Friday|     6|         Y|      1|\n",
      "|  2009-01-03|     2|20090103|2009-01-03|2009|   January|       1| Saturday|     7|         N|      1|\n",
      "|  2009-01-04|     3|20090104|2009-01-04|2009|   January|       1|   Sunday|     1|         N|      1|\n",
      "|  2009-01-05|     4|20090105|2009-01-05|2009|   January|       1|   Monday|     2|         Y|      1|\n",
      "|  2009-01-06|     5|20090106|2009-01-06|2009|   January|       1|  Tuesday|     3|         Y|      1|\n",
      "|  2009-01-07|     6|20090107|2009-01-07|2009|   January|       1|Wednesday|     4|         Y|      1|\n",
      "|  2009-01-08|     7|20090108|2009-01-08|2009|   January|       1| Thursday|     5|         Y|      1|\n",
      "|  2009-01-09|     8|20090109|2009-01-09|2009|   January|       1|   Friday|     6|         Y|      1|\n",
      "|  2009-01-10|     9|20090110|2009-01-10|2009|   January|       1| Saturday|     7|         N|      1|\n",
      "|  2009-01-11|    10|20090111|2009-01-11|2009|   January|       1|   Sunday|     1|         N|      1|\n",
      "|  2009-01-12|    11|20090112|2009-01-12|2009|   January|       1|   Monday|     2|         Y|      1|\n",
      "|  2009-01-13|    12|20090113|2009-01-13|2009|   January|       1|  Tuesday|     3|         Y|      1|\n",
      "|  2009-01-14|    13|20090114|2009-01-14|2009|   January|       1|Wednesday|     4|         Y|      1|\n",
      "|  2009-01-15|    14|20090115|2009-01-15|2009|   January|       1| Thursday|     5|         Y|      1|\n",
      "|  2009-01-16|    15|20090116|2009-01-16|2009|   January|       1|   Friday|     6|         Y|      1|\n",
      "|  2009-01-17|    16|20090117|2009-01-17|2009|   January|       1| Saturday|     7|         N|      1|\n",
      "|  2009-01-18|    17|20090118|2009-01-18|2009|   January|       1|   Sunday|     1|         N|      1|\n",
      "|  2009-01-19|    18|20090119|2009-01-19|2009|   January|       1|   Monday|     2|         Y|      1|\n",
      "|  2009-01-20|    19|20090120|2009-01-20|2009|   January|       1|  Tuesday|     3|         Y|      1|\n",
      "+------------+------+--------+----------+----+----------+--------+---------+------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:18:44.219088Z",
     "start_time": "2025-05-09T11:18:41.297024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#LOAD\n",
    "#dimDate.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"date_dim\")\n",
    "\n",
    "#parquet file\n",
    "dimDate.repartition(1).write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"dimDate_pq\")"
   ],
   "id": "8edb713e7d5c02ed",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:18:44.487693Z",
     "start_time": "2025-05-09T11:18:44.272619Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "5dfd9cce88203f6b",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
