{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T11:13:26.906086Z",
     "start_time": "2025-03-19T11:13:26.635912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions\n",
    "import ConnectionConfig as cc\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Set up environment\n",
    "cc.setupEnvironment()\n",
    "cc.listEnvironment()\n",
    "\n",
    "spark = cc.startLocalCluster(\"weather_dim\")\n",
    "spark.getActiveSession()\n",
    "\n",
    "cc.set_connectionProfile(\"veloDB\")\n",
    "\n",
    "df_operational_stations = (spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"driver\", cc.get_Property(\"driver\"))\n",
    "    .option(\"url\", cc.create_jdbc())\n",
    "    .option(\"dbtable\", \"stations\")\n",
    "    .option(\"user\", cc.get_Property(\"username\"))\n",
    "    .option(\"password\", cc.get_Property(\"password\"))\n",
    "    .load())\n",
    "\n",
    "zipcodes_iterator=(df_operational_stations\n",
    "                        .select(\"zipcode\").distinct()).toLocalIterator()\n",
    "zipcodes = list(zipcodes_iterator)\n",
    "\n",
    "print(zipcodes)\n"
   ],
   "id": "ec7d57243c6013fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLUSERSPROFILE: C:\\ProgramData\n",
      "APPDATA: C:\\Users\\dobis\\AppData\\Roaming\n",
      "COMMONPROGRAMFILES: C:\\Program Files\\Common Files\n",
      "COMMONPROGRAMFILES(X86): C:\\Program Files (x86)\\Common Files\n",
      "COMMONPROGRAMW6432: C:\\Program Files\\Common Files\n",
      "COMPUTERNAME: VIKI\n",
      "COMSPEC: C:\\WINDOWS\\system32\\cmd.exe\n",
      "DRIVERDATA: C:\\Windows\\System32\\Drivers\\DriverData\n",
      "GOPATH: C:\\Users\\dobis\\go\n",
      "HOMEDRIVE: C:\n",
      "HOMEPATH: \\Users\\dobis\n",
      "IGCCSVC_DB: AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAAAdGjjLHLGEWRGn9vRsCSowQAAAACAAAAAAAQZgAAAAEAACAAAAD7UhTq8CVvkaUfJ5fXTR5kgkvcIed3OfwPabt1yHYIgAAAAAAOgAAAAAIAACAAAABSFZMhyRZv+fj9Q44MNd0sMMQVbnBwNGmcsxiNFFrAcmAAAAAQjo+0swEYFhn4kypkFiEe0Z+EUeRh+XkMWaxY6J5h885R6WUpGPQjsBjQtDBiDzTJJu/Eu8HKO9rNDQ2HtHCLXjOrbciSueB80zvNehaNnexWcFALkN4Q37FMwos4go9AAAAAsBkth/vA4x8SgTkTjgM6mv3GKidgi5oDWFMyb92y29Ab+MuztrSDRGCJMXxOGeO0p8LJ8WZBYOU66GQTsD0WFw==\n",
      "IPY_INTERRUPT_EVENT: 2020\n",
      "JPY_INTERRUPT_EVENT: 2020\n",
      "JPY_PARENT_PID: 2028\n",
      "JPY_SESSION_NAME: DWH_WeatherDim.ipynb\n",
      "LANG: en_US.UTF-8\n",
      "LANGUAGE: \n",
      "LC_ALL: en_US.UTF-8\n",
      "LOCALAPPDATA: C:\\Users\\dobis\\AppData\\Local\n",
      "LOGONSERVER: \\\\VIKI\n",
      "NUMBER_OF_PROCESSORS: 12\n",
      "ONEDRIVE: C:\\Users\\dobis\\OneDrive - KdG\n",
      "ONEDRIVECOMMERCIAL: C:\\Users\\dobis\\OneDrive - KdG\n",
      "ONLINESERVICES: Online Services\n",
      "OS: Windows_NT\n",
      "PATH: C:\\KDG\\sparkdelta\\.venv\\Scripts;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\PuTTY\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\nodejs\\;C:\\Program Files\\Go\\bin;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Launcher\\;C:\\Users\\dobis\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\dobis\\AppData\\Local\\JetBrains\\Toolbox\\scripts;C:\\Users\\dobis\\.jdks\\temurin-20.0.2\\bin\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\dobis\\AppData\\Roaming\\npm;C:\\Users\\dobis\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\dobis\\go\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin\n",
      "PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n",
      "PLATFORMCODE: AN\n",
      "PROCESSOR_ARCHITECTURE: AMD64\n",
      "PROCESSOR_IDENTIFIER: Intel64 Family 6 Model 186 Stepping 3, GenuineIntel\n",
      "PROCESSOR_LEVEL: 6\n",
      "PROCESSOR_REVISION: ba03\n",
      "PROGRAMDATA: C:\\ProgramData\n",
      "PROGRAMFILES: C:\\Program Files\n",
      "PROGRAMFILES(X86): C:\\Program Files (x86)\n",
      "PROGRAMW6432: C:\\Program Files\n",
      "PROMPT: (.venv) $P$G\n",
      "PSMODULEPATH: %ProgramFiles%\\WindowsPowerShell\\Modules;C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\n",
      "PT8HOME: C:\\Program Files\\Cisco Packet Tracer 8.2.1\n",
      "PUBLIC: C:\\Users\\Public\n",
      "PYTHONPATH: C:\\KDG\\dataai_groupproject;C:/Users/dobis/AppData/Local/Programs/PyCharm Professional/plugins/python-ce/helpers/pydev;C:/Users/dobis/AppData/Local/Programs/PyCharm Professional/plugins/python/helpers-pro/jupyter_debug\n",
      "REGIONCODE: EMEA\n",
      "SESSIONNAME: Console\n",
      "SYSTEMDRIVE: C:\n",
      "SYSTEMROOT: C:\\WINDOWS\n",
      "TEMP: C:\\Users\\dobis\\AppData\\Local\\Temp\n",
      "TMP: C:\\Users\\dobis\\AppData\\Local\\Temp\n",
      "TOOLBOX_VERSION: 2.5.4.38621\n",
      "USERDOMAIN: VIKI\n",
      "USERDOMAIN_ROAMINGPROFILE: VIKI\n",
      "USERNAME: dobis\n",
      "USERPROFILE: C:\\Users\\dobis\n",
      "VBOX_MSI_INSTALL_PATH: C:\\Program Files\\Oracle\\VirtualBox\\\n",
      "VIRTUAL_ENV: C:\\KDG\\sparkdelta\\.venv\n",
      "WINDIR: C:\\WINDOWS\n",
      "ZES_ENABLE_SYSMAN: 1\n",
      "_OLD_VIRTUAL_PATH: C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\PuTTY\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\nodejs\\;C:\\Program Files\\Go\\bin;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Python\\Launcher\\;C:\\Users\\dobis\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\dobis\\AppData\\Local\\JetBrains\\Toolbox\\scripts;C:\\Users\\dobis\\.jdks\\temurin-20.0.2\\bin\\;C:\\Users\\dobis\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\dobis\\AppData\\Roaming\\npm;C:\\Users\\dobis\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\dobis\\go\\bin\n",
      "_OLD_VIRTUAL_PROMPT: $P$G\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "PYSPARK_PYTHON: python\n",
      "SPARK_HOME: C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\n",
      "HADOOP_HOME: C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\n",
      "PYSPARK_HADOOP_VERSION: 3\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-17\\\n",
      "SPARK_AUTH_SOCKET_TIMEOUT: 15\n",
      "SPARK_BUFFER_SIZE: 65536\n",
      "[Row(zipcode='2140'), Row(zipcode='2060'), Row(zipcode='2170'), Row(zipcode='2100'), Row(zipcode='2050'), Row(zipcode='2018'), Row(zipcode='2600'), Row(zipcode='2030'), Row(zipcode='2020'), Row(zipcode='2660'), Row(zipcode='2000'), Row(zipcode='2610')]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import openmeteo_requests\n",
    "\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "\t\"latitude\": 52.52,\n",
    "\t\"longitude\": 13.41,\n",
    "\t\"hourly\": [\"temperature_2m\", \"weather_code\", \"rain\"]\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()}{response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "\t\t\t\t\t\t\t# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_weather_code = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_rain = hourly.Variables(2).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "hourly_data[\"rain\"] = hourly_rain\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "print(hourly_dataframe)"
   ],
   "id": "cefb2376bd6416d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
