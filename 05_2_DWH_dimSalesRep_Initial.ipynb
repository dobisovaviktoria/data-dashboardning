{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Config stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, functions\n",
    "import ConnectionConfig as cc\n",
    "from pyspark.sql.functions import *\n",
    "cc.setupEnvironment()\n",
    "cc.listEnvironment()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T13:34:18.827177Z",
     "start_time": "2025-02-26T13:34:18.651248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLUSERSPROFILE: C:\\ProgramData\n",
      "APPDATA: C:\\Users\\User\\AppData\\Roaming\n",
      "COMMONPROGRAMFILES: C:\\Program Files\\Common Files\n",
      "COMMONPROGRAMFILES(X86): C:\\Program Files (x86)\\Common Files\n",
      "COMMONPROGRAMW6432: C:\\Program Files\\Common Files\n",
      "COMPUTERNAME: MSI\n",
      "COMSPEC: C:\\WINDOWS\\system32\\cmd.exe\n",
      "CONFIGSETROOT: C:\\WINDOWS\\ConfigSetRoot\n",
      "DRIVERDATA: C:\\Windows\\System32\\Drivers\\DriverData\n",
      "EFC_11108: 1\n",
      "HOMEDRIVE: C:\n",
      "HOMEPATH: \\Users\\User\n",
      "IPY_INTERRUPT_EVENT: 2024\n",
      "JPY_INTERRUPT_EVENT: 2024\n",
      "JPY_PARENT_PID: 2020\n",
      "JPY_SESSION_NAME: 05_2_DWH_dimSalesRep_Initial.ipynb\n",
      "LANG: en_US.UTF-8\n",
      "LANGUAGE: \n",
      "LC_ALL: en_US.UTF-8\n",
      "LOCALAPPDATA: C:\\Users\\User\\AppData\\Local\n",
      "LOGONSERVER: \\\\MSI\n",
      "NUMBER_OF_PROCESSORS: 20\n",
      "ONEDRIVE: C:\\Users\\User\\OneDrive\n",
      "ONEDRIVECONSUMER: C:\\Users\\User\\OneDrive\n",
      "OS: Windows_NT\n",
      "PATH: D:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\\Scripts;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Cloudflare\\Cloudflare WARP\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\PuTTY\\;D:\\KdG\\Programming 2\\NodeJs\\;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Launcher\\;C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\User\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\User\\AppData\\Local\\JetBrains\\Toolbox\\scripts;C:\\Users\\User\\AppData\\Roaming\\npm;C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\\bin;C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\\bin;C:\\Program Files\\Java\\jdk-17\\bin\n",
      "PATHEXT: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n",
      "PROCESSOR_ARCHITECTURE: AMD64\n",
      "PROCESSOR_IDENTIFIER: Intel64 Family 6 Model 154 Stepping 3, GenuineIntel\n",
      "PROCESSOR_LEVEL: 6\n",
      "PROCESSOR_REVISION: 9a03\n",
      "PROGRAMDATA: C:\\ProgramData\n",
      "PROGRAMFILES: C:\\Program Files\n",
      "PROGRAMFILES(X86): C:\\Program Files (x86)\n",
      "PROGRAMW6432: C:\\Program Files\n",
      "PROMPT: (venv) $P$G\n",
      "PSMODULEPATH: C:\\Program Files\\WindowsPowerShell\\Modules;C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\n",
      "PT8HOME: C:\\Program Files\\Cisco Packet Tracer 8.2.1\n",
      "PUBLIC: C:\\Users\\Public\n",
      "PYTHONPATH: D:\\KdG\\Year 2\\Sem2\\DataAI Sem 2\\sparkdelta\n",
      "SESSIONNAME: Console\n",
      "SYSTEMDRIVE: C:\n",
      "SYSTEMROOT: C:\\WINDOWS\n",
      "TEMP: C:\\Users\\User\\AppData\\Local\\Temp\n",
      "TMP: C:\\Users\\User\\AppData\\Local\\Temp\n",
      "USERDOMAIN: MSI\n",
      "USERDOMAIN_ROAMINGPROFILE: MSI\n",
      "USERNAME: User\n",
      "USERPROFILE: C:\\Users\\User\n",
      "VBOX_MSI_INSTALL_PATH: C:\\Program Files\\Oracle\\VirtualBox\\\n",
      "VIRTUAL_ENV: D:\\KdG\\Year 2\\Integration\\sparkdelta\\venv\n",
      "WINDIR: C:\\WINDOWS\n",
      "ZES_ENABLE_SYSMAN: 1\n",
      "_OLD_VIRTUAL_PATH: C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Cloudflare\\Cloudflare WARP\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\PuTTY\\;D:\\KdG\\Programming 2\\NodeJs\\;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Launcher\\;C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\User\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\User\\AppData\\Local\\JetBrains\\Toolbox\\scripts;C:\\Users\\User\\AppData\\Roaming\\npm\n",
      "_OLD_VIRTUAL_PROMPT: $P$G\n",
      "__PSLOCKDOWNPOLICY: 0\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "TERM: xterm-color\n",
      "CLICOLOR: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR_FORCE: 1\n",
      "PAGER: cat\n",
      "GIT_PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "PYSPARK_PYTHON: python\n",
      "SPARK_HOME: C:\\tools\\bigdatatools\\spark-3.5.2-bin-hadoop3\n",
      "HADOOP_HOME: C:\\tools\\bigdatatools\\hadoop-3.4.0-win10-x64\n",
      "PYSPARK_HADOOP_VERSION: 3\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-17\\\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"dimSalesRepInit\")\n",
    "spark.getActiveSession()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T13:34:28.430685Z",
     "start_time": "2025-02-26T13:34:18.928281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23b4202a690>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.140.65.233:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dimSalesRepInit</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the operational database\n",
    "In order to run this demo you have to create a tutorial_op database and run the PostgreSQL_SalesOperational.sql script.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initial load\n",
    "We will create a slowly changing dimension type 2 called dimSalesRep based on a sourceTable in our operational database called dbo.salesrep. SCD2  tables demand extra care and  because we will store hirstorical values of the dimension with the help of ranges.\n",
    "This notebook will create the table and fill it with the initial data. A second notebook will be used for increments of new and changed data.\n",
    "\n",
    "This is an example of the expected output (salesRepSK is different\n",
    "```\n",
    "+----------+-------------+-------------+-----------+-------------------+-------------------+--------------------+-------+\n",
    "|salesRepID|         name|       office| salesRepSK|          scd_start|            scd_end|                 md5|current|\n",
    "+----------+-------------+-------------+-----------+-------------------+-------------------+--------------------+-------+\n",
    "|a46add1...|      Z. Jane|     New York|          0|1990-01-01 00:00:00|2100-12-12 00:00:00|303db545462092a92...|   true|\n",
    "|s1fedf1...|   P. Chapman|       Berlin|          1|1990-01-01 00:00:00|2100-12-12 00:00:00|14b094c31bf9e4149...|   true|\n",
    "|d5e6f77...|     T. Crane|     New York|          2|1990-01-01 00:00:00|2100-12-12 00:00:00|6c062f95defda9dc3...|   true|\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cc.set_connectionProfile(\"tutorial_op\")\n",
    "\n",
    "df_operational_sales_rep = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"dbtable\", \"salesrep\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"salesRepID\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 20) \\\n",
    "    .load()\n",
    "\n",
    "#Method 1 : Use the DataFrame API\n",
    "#df_dim_sales_rep = df_sales_rep.withColumn(\"salesRepSK\", expr(\"uuid()\")) \\\n",
    "#    .withColumn(\"scd_start\", lit(\"1990-01-01\").cast(\"timestamp\")) \\\n",
    "#    .withColumn(\"scd_end\", lit(\"2100-12-12\").cast(\"timestamp\")) \\\n",
    "#    .withColumn(\"md5\", md5(concat( col('name'), col('office')))) \\\n",
    "#    .withColumn(\"current\", lit(True))\n",
    "\n",
    "#Method 2 : Use SQL\n",
    "df_operational_sales_rep.createOrReplaceTempView(\"dimSalesRep\")\n",
    "df_dim_sales_rep = spark.sql(\"select uuid() as salesRepSK, *, to_timestamp('1999-01-01','yyyy-MM-dd') as scd_start, to_timestamp('2100-12-12','yyyy-MM-dd') as scd_end, md5(concat( name, office)) as md5, True as current  from dimSalesRep\")\n",
    "\n",
    "df_dim_sales_rep.printSchema()\n",
    "df_dim_sales_rep.show()\n",
    "df_dim_sales_rep.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimSalesRep\")\n",
    "\n",
    "#spark.sql(\"ALTER TABLE dimSalesRep  ADD columns (salesRepSK long GENERATED ALWAYS AS IDENTITY (START WITH 0 INCREMENT BY 1)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T13:34:41.533945Z",
     "start_time": "2025-02-26T13:34:30.906977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- salesRepSK: string (nullable = false)\n",
      " |-- salesrepid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- office: string (nullable = true)\n",
      " |-- scd_start: timestamp (nullable = true)\n",
      " |-- scd_end: timestamp (nullable = true)\n",
      " |-- md5: string (nullable = true)\n",
      " |-- current: boolean (nullable = false)\n",
      "\n",
      "+--------------------+----------+-------------+-------------+-------------------+-------------------+--------------------+-------+\n",
      "|          salesRepSK|salesrepid|         name|       office|          scd_start|            scd_end|                 md5|current|\n",
      "+--------------------+----------+-------------+-------------+-------------------+-------------------+--------------------+-------+\n",
      "|7a9cb5e0-a4bf-4cf...|         1|      R. Zane|       Berlin|1999-01-01 00:00:00|2100-12-12 00:00:00|1f8cbbc272a33dcc1...|   true|\n",
      "|4a1e3ee6-6abc-41d...|         2|   P. Chapman|       Berlin|1999-01-01 00:00:00|2100-12-12 00:00:00|14b094c31bf9e4149...|   true|\n",
      "|411aef47-48f0-422...|         3|     F. Crane|     New York|1999-01-01 00:00:00|2100-12-12 00:00:00|0715f05df18a3a794...|   true|\n",
      "|675dd7ae-28a9-401...|         4|    R. Geller|     New York|1999-01-01 00:00:00|2100-12-12 00:00:00|6212c0ce01f144d66...|   true|\n",
      "|75863890-e2bd-4bb...|         5|     T. Mosby|       Berlin|1999-01-01 00:00:00|2100-12-12 00:00:00|947579dec8084039e...|   true|\n",
      "|07e200cd-916c-46b...|         6|   H. Simpson|       Berlin|1999-01-01 00:00:00|2100-12-12 00:00:00|d636d1b0685650b34...|   true|\n",
      "|e58e729e-bb8c-443...|         7|   B. Stinson|San Fransisco|1999-01-01 00:00:00|2100-12-12 00:00:00|e726b2d8dc0cf9a6f...|   true|\n",
      "|b4f2d44e-1333-46c...|         8|L. Hofstadter|     Brussels|1999-01-01 00:00:00|2100-12-12 00:00:00|a2bbe52f8274b0f08...|   true|\n",
      "|9929c9f6-064c-49f...|         9|    S. Cooper|     Brussels|1999-01-01 00:00:00|2100-12-12 00:00:00|d85c73c9d03df0002...|   true|\n",
      "|3a82f97e-33bc-4ea...|        10| F. Underwood|     Brussels|1999-01-01 00:00:00|2100-12-12 00:00:00|44cd1a6d596b05688...|   true|\n",
      "|4c87d6a1-2714-4a9...|        11|     W. White|     New York|1999-01-01 00:00:00|2100-12-12 00:00:00|f9ea69ce2aa4482b4...|   true|\n",
      "|a15d5371-53b9-49c...|        12| T. Lannister|     New York|1999-01-01 00:00:00|2100-12-12 00:00:00|3259a471f9816d7c3...|   true|\n",
      "|f4330caf-f064-434...|        13|      M. Ross|       London|1999-01-01 00:00:00|2100-12-12 00:00:00|d0faf94c1bbe2d4a7...|   true|\n",
      "+--------------------+----------+-------------+-------------+-------------------+-------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The function lit() is used when you want a fixed column value for every row. In this case scd_start, scd_end and current.\n",
    "* The function md5() performs a md5-hash function on the preferred columns. This is needed to detect scd2 changes. When one of the included columns changes, the md5-hash will change. Include all SCD2 columns in the md5-hash function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Delete the spark session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T13:35:18.667224Z",
     "start_time": "2025-02-26T13:35:18.660603Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T13:34:41.726031Z",
     "start_time": "2025-02-26T13:34:41.721962Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
